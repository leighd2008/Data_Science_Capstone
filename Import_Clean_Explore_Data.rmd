---
title: "Import_Explore_Data"
author: "Diane Leigh"
date: "September 3, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# ipak function: install and load multiple R packages.
# check to see if packages are installed. Install them if they are not, then load them into the R session.

ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}
packages <- c("tidyverse", "readtext", "tm", "RColorBrewer", "SnowballC", "RWeka", "RWekajars", "NLP", "openNLP","stringi", "rJava", "R.utils", "stringr", "tidytext", "scales")
ipak(packages)

```
## Download SwiftKey data for the Data Science Specialization Capstone project.

```{r load_data, results="asis", cache=TRUE}
## Read training and data set
fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

if(!file.exists("Coursera-Swiftkey.zip")){
  temp <- "Coursera-Swiftkey.zip"
  download.file(fileUrl, temp)
  DateDataDownloaded <- date()
  saveRDS(DateDataDownloaded, file="DateDataDownloaded.rds")
  unzip(temp)
}else {
  DateDataDownloaded <- readRDS("DateDataDownloaded.rds")
} 

```

Data was downloaded `r DateDataDownloaded`

## Import data

Data has been downloaded and unzipped into the Data_Science_Capstone_Project folder. Here we use the rbinom function to sample 2% of the lines in each file for exploratory analysis.

```{r import, warning=FALSE}
if(!file.exists("./datafiles/cordata.RData")){
  ds <- DirSource("./final/en_US")
  drfnm <- dir("./final/en_US")
  flInfo <- data.frame()
  sampInfo <- data.frame()
    j=0
  for(i in drfnm) {
    j=j+1
    flInfo[j,1] <- drfnm[j]
    flInfo[j,2] <- file.info(ds$filelist[j])$size / 1024^2 # Megabytes
    fnm <- file(ds$filelist[j], open="rb")
    y <- readLines(fnm, encoding = "UTF-8", skipNul=TRUE)
    flInfo[j,3] <- length(y)
    close(fnm)
    set.seed(3948)
    x <- y[rbinom(n = length(y), size = 1, prob = 0.02) == 1]
    flInfo[j,4] <- sum(stri_count_words(y))
    flInfo[j,5] <- max(nchar(y))
    if (grepl(".blog.*", i)){
      assign("blogs",x) 
      sampInfo[j,1] <- "blogs"
    }
    else if (grepl(".news.*", i)){
      assign("news",x)
      sampInfo[j,1] <- "news"
    }
    else if (grepl(".twitter.*", i)){
      assign("twitter",x) 
      sampInfo[j,1] <- "twitter"
    }
    sampInfo[j,2] <- length(x)
    sampInfo[j,3] <- sum(stri_count_words(x))
    sampInfo[j,4] <- max(nchar(x))
  }
  colnames(flInfo) <- c("File_Name", "Size_(MB)", "Num_of_Lines", "Num_of_Words", "Max_Characters")
  colnames(sampInfo) <- c("File_Name", "Num_of_Lines", "Num_of_Words", "Max_Characters")
  #combine sampled lines from blogs, news and twitter into one corpus
  cordata <- c(blogs, news, twitter)
  #save the data sampled data to RData files
  mkdirs("datafiles")
  saveRDS(blogs, "./datafiles/blogs.RData")
  saveRDS(news, "./datafiles/news.RData")
  saveRDS(twitter, "./datafiles/twitter.RData")
  saveRDS(cordata, "./datafiles/cordata.RData")
  saveRDS(flInfo, file = "./datafiles/flInfo.RData")
  saveRDS(sampInfo, file = "./datafiles/sampInfo.RData")
  # clean environment
  rm(drfnm, ds, fnm, i, j, x, y)
}else {
  cordata <- readRDS("./datafiles/cordata.RData")
  blogs <- readRDS("./datafiles/blogs.RData")
  news <- readRDS("./datafiles/news.RData")
  twitter <- readRDS("./datafiles/twitter.RData")
  flInfo <- readRDS("./datafiles/flInfo.RData")
  sampInfo <- readRDS("./datafiles/sampInfo.RData")
}

```

```{r tidydata}
# convert variables to data frames, one line per row
blogs_df <- data_frame(text = blogs) %>% mutate(linenumber = row_number())
news_df <- data_frame(text = news) %>% mutate(linenumber = row_number())
twitter_df <- data_frame(text = twitter) %>% mutate(linenumber = row_number())

# convert variables to one word per line (tokenization)
data("stop_words")
tidy_blog <- blogs_df %>% unnest_tokens(word, text) %>% anti_join(stop_words)
tidy_news <- news_df %>% unnest_tokens(word, text) %>% anti_join(stop_words)
tidy_twitter <- twitter_df %>% unnest_tokens(word,text)%>%anti_join(stop_words) 
```


```{r plot_word_count}
# plot word frequencies
blog_frq <- tidy_blog %>% count(word, sort = TRUE) %>% filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>% ggplot(aes(word, n)) + geom_col() +
  xlab(NULL) + ggtitle("The most common words in our blog sample") +
  coord_flip()

news_frq <- tidy_news %>% count(word, sort = TRUE) %>% filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>% ggplot(aes(word, n)) + geom_col() +
  xlab(NULL) + ggtitle("The most common words in our news sample") + 
  coord_flip()

twitter_frq <- tidy_twitter %>% count(word, sort = TRUE) %>% 
  filter(n > 600) %>% mutate(word = reorder(word, n)) %>% 
  ggplot(aes(word, n)) + geom_col() + xlab(NULL) +
  ggtitle("The most common words in our twitter sample") + coord_flip()

blog_frq
news_frq
twitter_frq

```

```{r plot_comparison, warnings=FALSE}
frequency <- bind_rows(mutate(tidy_blog, Text_Source = "Blogs"),
  mutate(tidy_twitter, Text_Source = "Twitter"), 
  mutate(tidy_news, Text_Source = "xNews")) %>% 
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(Text_Source, word) %>% group_by(Text_Source) %>%
  mutate(proportion = n / sum(n)) %>% select(-n) %>% 
  spread(Text_Source, proportion) %>% 
  gather(Text_Source, proportion, `Blogs`:`Twitter`)

freq_comp <- ggplot(frequency, aes(x = proportion, y = `xNews`, 
  color = abs(`xNews` - proportion))) + 
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", 
                       high = "gray75") +
  facet_wrap(~Text_Source, ncol = 2) + theme(legend.position="none") +
  labs(y = "News", x = NULL)

suppressWarnings(print(freq_comp))

```

