---
title: "Import_Explore_Data"
author: "Diane Leigh"
date: "September 3, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# ipak function: install and load multiple R packages.
# check to see if packages are installed. Install them if they are not, then load them into the R session.

ipak <- function(pkg){
    new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
    if (length(new.pkg)) 
        install.packages(new.pkg, dependencies = TRUE)
    sapply(pkg, require, character.only = TRUE)
}

packages <- c("tidyverse", "readtext", "tm", "RColorBrewer", "SnowballC", "RWeka", "RWekajars", "NLP", "openNLP","stringi", "rJava", "R.utils", "stringr", "tidytext", "scales", "wordnet", "igraph", "ggraph", "quanteda", "data.table")

ipak(packages)

```
## Download SwiftKey data for the Data Science Specialization Capstone project.

```{r load_data, results="asis", cache=TRUE}
## Read training and data set
fileUrl <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"

if(!file.exists("Coursera-Swiftkey.zip")){
  temp <- "Coursera-Swiftkey.zip"
  download.file(fileUrl, temp)
  DateDataDownloaded <- date()
  saveRDS(DateDataDownloaded, file="DateDataDownloaded.rds")
  unzip(temp)
}else {
  DateDataDownloaded <- readRDS("DateDataDownloaded.rds")
} 

```

Data was downloaded `r DateDataDownloaded`

## Import data

Data has been downloaded and unzipped into the Data_Science_Capstone_Project folder. Here we use the rbinom function to sample 2% of the lines in each file for exploratory analysis.

```{r import, warning=FALSE}
if(!file.exists("./datafiles/corpusdata.RData")){
  ds <- DirSource("./final/en_US")
  drfnm <- dir("./final/en_US")
  flInfo <- data.frame()
  sampInfo <- data.frame()
    j=0
  for(i in drfnm) {
    j=j+1
    flInfo[j,1] <- drfnm[j]
    flInfo[j,2] <- file.info(ds$filelist[j])$size / 1024^2 # Megabytes
    fnm <- file(ds$filelist[j], open="rb")
    y <- readLines(fnm, encoding = "UTF-8", skipNul=TRUE)
    flInfo[j,3] <- length(y)
    close(fnm)
    set.seed(3948)
    x <- y[rbinom(n = length(y), size = 1, prob = 0.02) == 1]
    flInfo[j,4] <- sum(stri_count_words(y))
    flInfo[j,5] <- max(nchar(y))
    if (grepl(".blog.*", i)){
      assign("blogs",x) 
      sampInfo[j,1] <- "blogs"
    }
    else if (grepl(".news.*", i)){
      assign("news",x)
      sampInfo[j,1] <- "news"
    }
    else if (grepl(".twitter.*", i)){
      assign("twitter",x) 
      sampInfo[j,1] <- "twitter"
    }
    sampInfo[j,2] <- length(x)
    sampInfo[j,3] <- sum(stri_count_words(x))
    sampInfo[j,4] <- max(nchar(x))
  }
  colnames(flInfo) <- c("File_Name", "Size_(MB)", "Num_of_Lines", "Num_of_Words", "Max_Characters")
  colnames(sampInfo) <- c("File_Name", "Num_of_Lines", "Num_of_Words", "Max_Characters")
  #combine sampled lines from blogs, news and twitter into one corpus
  mycorpusblogs <- corpus(blogs)
  docvars(mycorpusblogs, "Source") <- "Blog"
  mycorpusnews <- corpus(news)
  docvars(mycorpusnews, "Source") <- "News"
  mycorpustwitter <- corpus(twitter)
  docvars(mycorpustwitter, "Source") <- "Tweet"
  corpusdata <- mycorpusblogs + mycorpusnews + mycorpustwitter
  #save the sampled data to RData files
  mkdirs("datafiles")
  saveRDS(mycorpusblogs, "./datafiles/mycorpusblogs.RData")
  saveRDS(mycorpusnews, "./datafiles/mycorpusnews.RData")
  saveRDS(mycorpustwitter, "./datafiles/mycorpustwitter.RData")
  saveRDS(corpusdata, "./datafiles/corpusdata.RData")
  saveRDS(flInfo, file = "./datafiles/flInfo.RData")
  saveRDS(sampInfo, file = "./datafiles/sampInfo.RData")
  #clean environment
  rm(drfnm, ds, fnm, i, j, x, y, news, twitter, blogs, mycorpusblogs,
     mycorpusnews, mycorpustwitter)
}else {
  corpusdata <- readRDS("./datafiles/corpusdata.RData")
  #mycorpusblogs <- readRDS("./datafiles/mycorpusblogs.RData")
  #mycorpusnews <- readRDS("./datafiles/mycorpusnews.RData")
  #mycorpustwitter <- readRDS("./datafiles/mycorpustwitter.RData")
  flInfo <- readRDS("./datafiles/flInfo.RData")
  sampInfo <- readRDS("./datafiles/sampInfo.RData")
}

```


```{r tidydata}
# convert variables to data frames, one token(word) per row, remove profanity
# numbers, punctuation, symbols, separators hashtags and URLs.

if(!file.exists("profanity.txt")){
  fileUrl<- "https://www.cs.cmu.edu/~biglou/resources/bad-words.txt"
  temp <- "profanity.txt"
  download.file(fileUrl, temp)
}

profanity <- readLines("profanity.txt")

totdfm <- dfm(corpusdata, remove = profanity,
       remove_numbers = TRUE, remove_punct = TRUE, removeSymbols=TRUE,
       remove_separators = TRUE, removeTwitter=TRUE, remove_url=TRUE,
       include_docvars = TRUE)

totfrq <- as.data.table(docfreq(totdfm), keep.rownames = TRUE) %>%  
     setnames(c("V1","V2"), c("word", "n")) %>% setorder(-n)

# plot word frequencies

tot_frq <- totfrq %>% filter(n > 5000) %>% mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() +
  ggtitle("Words with frequency > 5000 in our courpus") 

tot_frq

```



```{r bigrams}

# convert variables to one bigram per line (tokenization)
bi_totdfm <- dfm(corpusdata, remove = profanity, ngrams = 2,
       remove_numbers = TRUE, remove_punct = TRUE, removeSymbols=TRUE,
       remove_separators = TRUE, removeTwitter=TRUE, remove_url=TRUE,
       include_docvars = TRUE)

bi_tot <- as.data.table(docfreq(bi_totdfm), keep.rownames = TRUE) %>%  
  setnames(c("V1","V2"), c("word", "n")) %>% setorder(-n)


# plot bi-gram frequencies
bi_tot_frq <- bi_tot %>% filter(n > 1500) %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() +
  ggtitle("Bi-grams with frequency > 1500 in our courpus")

bi_tot_frq

# separate bigram words to separate columns 

bi_tot <- separate(bi_tot, word, c("word1", "word2"), sep = "_") 

```


```{r trigrams}

# convert variables to one trigram per line (tokenization)
tri_totdfm <- dfm(corpusdata, remove = profanity, ngrams = 3,
       remove_numbers = TRUE, remove_punct = TRUE, removeSymbols=TRUE,
       remove_separators = TRUE, removeTwitter=TRUE, remove_url=TRUE,
       include_docvars = TRUE)

tri_tot <- as.data.table(docfreq(tri_totdfm), keep.rownames = TRUE) %>%  
  setnames(c("V1","V2"), c("word", "n")) %>% setorder(-n)


# plot tri-gram frequencies
tri_tot_frq <- tri_tot %>% filter(n > 200) %>% 
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() +
  ggtitle("Tri-grams with frequency > 200 in our courpus")

tri_tot_frq

# separate trigram words to separate columns 

tri_tot <- separate(tri_tot, word, c("word1", "word2"), sep = "_") 

```
